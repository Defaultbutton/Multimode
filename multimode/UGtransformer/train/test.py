import torch
import torch.nn as nn
import torch.nn.functional as F
def cross_entropy(pred, soft_targets): # use nn.CrossEntropyLoss if not using soft labels in Line 159
    logsoftmax = nn.LogSoftmax(dim=1)
    return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))
# criterion = nn.CrossEntropyLoss()
if __name__ == '__main__':
    p = torch.tensor([-1.0037, -0.2152, -0.4221, -1.0872, 0.4345])
    s = torch.tensor([[0., 0., 0., 0., 1.]])
    result = cross_entropy(torch.unsqueeze(p, 0),s)
    print(result)
    la = torch.tensor([1,1,2,3])
    pr = torch.tensor([[1],[2],[1],[2]
                      ])
    lc = la.view_as(pr)
    print(lc)
    idx = [1,2,5,4,3]
    #train_idx, test_idx = idx[1]
    #prediction_score: tensor([-1.0037, -0.2152, -0.4221, -1.0872, 0.4345], device='cuda:0',)
    #graph_label: tensor([[0., 0., 0., 0., 1.]], device='cuda:0')
    node_feature = torch.tensor([[ 2.5832e+01],
        [-1.0257e+01],
        [-2.8514e+04],
        [-5.7707e+03],
        [-2.6736e+03],
        [-1.5925e+04],
        [-6.8303e+03],
        [ 2.3787e+02],
        [-1.1655e+01],
        [-9.4045e+02],
        [ 4.0062e+01],
        [-3.8761e+00],
        [ 1.7474e+03],
        [ 1.0146e+03],
        [ 1.3459e+03],
        [ 2.2406e+03],
        [ 2.2823e+03],
        [ 1.3177e+03],
        [ 3.7715e+03],
        [ 1.0148e+03],
        [ 2.0792e+03],
        [ 2.1942e+02],
        [-9.4974e+02],
        [ 5.2522e+01],
        [ 1.6164e+03],
        [-2.2301e+00],
        [-2.0973e+00],
        [-8.3717e+00],
        [ 1.5200e+02],
        [ 3.0796e+00],
        [ 1.4779e+01],
        [ 5.2743e+00],
        [ 2.2212e+00],
        [ 1.0940e+03],
        [ 7.4319e+02],
        [ 2.9498e+03],
        [ 6.0563e+02],
        [ 8.2765e+02],
        [ 6.0521e+02],
        [ 1.1880e+03],
        [ 1.0122e+03],
        [ 7.4978e+02],
        [ 4.0124e+02],
        [ 1.3428e+03],
        [ 4.3207e+02],
        [ 1.3858e+03],
        [ 1.0030e+04],
        [ 2.3742e+03],
        [ 7.0429e+02],
        [ 7.0988e+02],
        [ 5.1992e+02],
        [ 1.1903e+03],
        [ 2.1595e+03],
        [ 1.2641e+03],
        [ 1.1942e+03],
        [ 1.2745e+03],
        [ 1.3063e+03],
        [ 3.2268e+02],
        [ 5.3798e+03],
        [ 1.8224e+03],
        [ 8.8349e+03],
        [-5.3388e+03],
        [ 1.6549e+00],
        [-4.3306e+03],
        [-1.1534e+03],
        [ 2.8694e+02],
        [ 2.7352e+05],
        [ 2.3373e+02],
        [ 1.0082e+03],
        [ 1.8278e+03],
        [ 1.3243e+03],
        [ 8.9696e+02],
        [ 1.3564e+03],
        [ 1.7072e+02],
        [ 5.1330e+02],
        [-1.6496e+03],
        [-5.2212e-01],
        [ 1.9248e+03],
        [ 8.5609e+02],
        [ 4.6949e+02],
        [ 2.2426e+03],
        [ 4.8288e+02],
        [-1.8487e+03],
        [ 1.9148e+03],
        [ 6.7113e+02],
        [ 1.5784e+03],
        [ 4.6819e+02],
        [ 2.4406e+04],
        [ 3.8311e+02],
        [ 3.5626e+02],
        [ 8.7959e+02],
        [ 6.9539e+02],
        [ 3.0371e+02],
        [ 6.4440e+02],
        [ 1.3281e+03],
        [ 5.0575e+02],
        [-8.9381e-01],
        [-1.4455e+02],
        [ 2.0354e-01],
        [-3.3308e+02],
        [-4.3727e+02],
        [-9.7811e+03],
        [ 9.5310e+00],
        [ 1.3894e+00],
        [-1.1403e+03],
        [-1.4218e+02],
        [ 2.6513e+01],
        [ 7.8769e+02],
        [-1.2035e+00],
        [ 1.0088e+00],
        [ 4.5196e+02],
        [-1.0885e+00],
        [-1.0628e+01],
        [-1.2389e+01],
        [ 2.3113e+02],
        [ 1.4531e+02],
        [-5.0708e+00],
        [-1.5735e+03],
        [-1.7919e+02],
        [ 8.0743e+02],
        [ 1.9801e+03],
        [ 1.0681e+03],
        [ 4.6796e+01],
        [ 5.2135e+02],
        [ 3.6700e+02],
        [ 8.9240e+02],
        [ 4.0959e+02],
        [ 2.5941e+03],
        [ 1.5279e+03],
        [ 9.3840e+02],
        [ 4.1303e+02],
        [-8.3896e+04],
        [ 1.0626e+04],
        [ 3.7382e+02],
        [ 3.6360e+02],
        [ 3.2062e+02],
        [-2.2301e+00],
        [ 4.7177e+01],
        [ 7.4062e+02],
        [ 4.6769e+02],
        [ 8.4129e+02],
        [ 1.3658e+02],
        [ 1.2341e+02],
        [ 1.6366e+02],
        [ 5.4212e+01],
        [-1.0363e+01],
        [ 8.8168e+02],
        [ 4.5398e+01],
        [ 4.1770e+00],
        [ 1.5487e+00],
        [ 1.1589e+02],
        [ 7.6460e+00],
        [-2.8912e+01],
        [ 1.6292e+01],
        [ 7.4336e-01],
        [-2.4602e+00],
        [-9.2035e-01],
        [ 6.6324e+02],
        [ 1.5582e+02],
        [-4.6405e+02],
        [ 1.0173e+04],
        [ 6.1973e+02],
        [ 9.4129e+03],
        [-3.4101e+02],
        [ 3.1903e+01],
        [ 3.4357e+04],
        [-1.7599e+03],
        [-1.1504e-01],
        [ 1.7345e+00],
        [ 1.0110e+03],
        [-1.1133e+01],
        [-2.4527e+02],
        [ 4.9038e+02],
        [-9.7345e-02],
        [ 4.6726e+00],
        [-2.0099e+04],
        [ 6.7861e+02],
        [-2.7699e+03],
        [-1.8288e+04],
        [-5.3824e+02],
        [-1.0965e+01],
        [ 2.1726e+02],
        [-1.8735e+01],
        [ 3.2212e+00],
        [-2.1407e+01],
        [ 2.9646e+00],
        [ 4.6974e+02],
        [ 1.6549e+00],
        [-3.2743e-01],
        [ 9.1478e+01],
        [ 1.6903e+00],
        [-9.4336e+00],
        [ 4.1150e+00],
        [ 8.0873e+02],
        [ 4.2398e+01],
        [-9.5664e+00],
        [ 1.1414e+03],
        [ 2.8319e-01],
        [ 4.2142e+02],
        [ 9.2035e-01],
        [-4.1062e+00],
        [-6.6726e+00],
        [-4.4248e-02],
        [ 2.7469e+01],
        [ 8.7611e-01],
        [-1.1504e-01],
        [-1.5929e-01],
        [ 1.3035e+01],
        [ 6.2881e+02],
        [-1.5575e+00],
        [ 9.8885e+01],
        [-8.4635e+02],
        [ 3.6460e+00],
        [-3.8673e+00],
        [-1.1841e+01],
        [ 5.4903e+02],
        [ 2.6375e+02],
        [-8.7697e+03],
        [ 2.1903e+01],
        [ 3.4513e-01],
        [-1.6549e+00],
        [ 1.6967e+02],
        [ 4.9175e+04],
        [-7.4204e+02],
        [ 2.8566e+01],
        [-2.7761e+01],
        [-1.0083e+03],
        [-3.1888e+02],
        [-1.8778e+03],
        [-1.0807e+04],
        [-2.0761e+02],
        [ 1.6942e+02],
        [-1.0973e+00],
        [-2.1239e-01],
        [-1.7788e+00],
        [-3.0177e+00],
        [-8.0587e+02],
        [ 1.2550e+02],
        [ 1.4137e+03],
        [ 2.3773e+02],
        [ 1.5457e+02],
        [ 1.8345e+01],
        [-1.0000e+00],
        [ 3.4071e+00],
        [ 8.4779e+00],
        [-8.8496e-02],
        [-6.1947e-01],
        [-2.4159e+00],
        [ 1.1827e+02],
        [-3.9646e+00],
        [-2.5703e+02],
        [-1.1546e+03],
        [-1.4071e+00],
        [-1.6796e+01],
        [-9.5604e+02],
        [ 9.2442e+01],
        [ 4.3381e+02],
        [ 5.3250e+02],
        [ 4.4796e+01],
        [-2.4208e+03],
        [-3.5233e+03],
        [ 7.5903e+01],
        [ 3.9823e-01]])
    node_feature_1 = torch.randn([10,1,512])
    #node_feature_1 = torch.unsqueeze(node_feature_1, 1)
    #print(node_feature.shape)
    encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=1)
    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)
    out = transformer_encoder(node_feature_1)
    #print(out)
    bn = torch.nn.BatchNorm1d(1)
    lin = nn.Linear(1,4)
    import torch.nn.functional as F

    # 假设你有未归一化的 y_score 张量
    y_score = torch.tensor([[0.2, 0.3, 0.5],
                            [0.4, 0.3, 0.3],
                            [0.7, 0.2, 0.1]])

    # 使用 PyTorch 的 softmax 函数进行归一化
    y_score_normalized = F.softmax(y_score, dim=0)

    print("Original y_score:")
    print(y_score)
    print("\nNormalized y_score:")
    print(y_score_normalized)




